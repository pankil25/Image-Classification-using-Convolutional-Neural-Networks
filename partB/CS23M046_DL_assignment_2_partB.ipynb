{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8044984,"sourceType":"datasetVersion","datasetId":4743635}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import ConcatDataset\nimport torch.nn.functional as F\nimport torchvision.models as models\nfrom PIL import Image\nimport os\nimport random\nfrom collections import defaultdict  # Import defaultdict\nimport numpy as np\nfrom torch.utils.data import random_split\nfrom torch.utils.data import Subset\nimport matplotlib.pyplot as plt\nimport wandb\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","id":"kaTaHUexuyO5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class FineTunedResNet(nn.Module):\n    def __init__(self, num_classes, dense_neurons, activation='relu', batch_normalization=False,\n                 dropout_value=0, freeze_option=None, freeze_index=0):\n        \"\"\"\n        Fine-tuned ResNet model for transfer learning.\n\n        Args:\n            num_classes (int): Number of output classes.\n            dense_neurons (int): Number of neurons in the dense layer.\n            activation (str, optional): Activation function for the dense layer. Defaults to 'relu'.\n            batch_normalization (bool, optional): Whether to include batch normalization. Defaults to False.\n            dropout_value (float, optional): Dropout probability for the dropout layer. Defaults to 0.\n            freeze_option (str, optional): Option to freeze layers. Defaults to None.\n            freeze_index (int, optional): Index up to which layers are frozen. Defaults to 0.\n        \"\"\"\n        super(FineTunedResNet, self).__init__()\n\n        # Load pre-trained ResNet model\n        resnet = models.resnet50(pretrained=True)\n\n        # Determine the number of layers in the ResNet model\n        num_layers = len(list(resnet.children()))\n\n        # Freeze layers based on the specified option\n        if freeze_option == 'all_except_last':\n            for param in resnet.parameters():\n                param.requires_grad = False\n            for param in resnet.fc.parameters():\n                param.requires_grad = True\n\n        elif freeze_option == 'up_to':\n            # Freeze layers up to a certain index (excluding the last layer)\n            if isinstance(freeze_index, int) and freeze_index < num_layers - 1:\n                for idx, child in enumerate(resnet.children()):\n                    if idx < freeze_index:\n                        for param in child.parameters():\n                            param.requires_grad = False\n                    else:\n                        break\n            else:\n                raise ValueError(\"Invalid 'freeze_index' value. It should be an integer less than the number of layers.\")\n\n        elif freeze_option == 'except_first':\n            # Freeze all layers except the first convolutional layer\n            for idx, child in enumerate(resnet.children()):\n                if idx == 0:\n                    continue\n                for param in child.parameters():\n                    param.requires_grad = False\n\n        elif freeze_option == 'fine_tuning':\n            pass\n\n        # Modify the last fully connected layer to match the number of output features\n        num_ftrs = resnet.fc.in_features\n        resnet.fc = nn.Linear(num_ftrs, dense_neurons)\n\n        # Conditionally add batch normalization\n        if batch_normalization:\n            self.batch_norm = nn.BatchNorm1d(dense_neurons)\n        else:\n            self.batch_norm = None\n\n        # Conditionally add dropout\n        if dropout_value > 0:\n            self.dropout = nn.Dropout(dropout_value)\n        else:\n            self.dropout = None\n\n        # Activation function\n        if activation == 'relu':\n            self.act = nn.ReLU()\n        elif activation == 'gelu':\n            self.act = nn.GELU()\n        elif activation == 'silu':\n            self.act = nn.SiLU()\n        elif activation == 'mish':\n            self.act = Mish()\n        else:\n            raise ValueError(\"Invalid activation function specified.\")\n\n        # Add a new fully connected layer for the number of classes\n        self.fc = nn.Linear(dense_neurons, num_classes)\n\n        self.resnet = resnet\n\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the model.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        x = self.resnet(x)\n\n        if self.batch_norm is not None:\n            x = self.batch_norm(x)\n            x = self.act(x)\n\n        if self.dropout is not None:\n            x = self.dropout(x)\n\n        x = self.fc(x)\n        return x\n\nclass Mish(nn.Module):\n    def forward(self, x):\n        \"\"\"\n        Forward pass of the Mish activation function.\n\n        Args:\n            x (torch.Tensor): Input tensor.\n\n        Returns:\n            torch.Tensor: Output tensor.\n        \"\"\"\n        return x * torch.tanh(F.softplus(x))\n","metadata":{"id":"WClIOJ3euyO9","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def data_loader(train_data_folder, batch_size, data_augmentation):\n    \"\"\"\n    Create PyTorch data loaders for training and validation datasets.\n\n    Args:\n        train_data_folder (str): Path to the training data folder.\n        batch_size (int): Batch size for the data loaders.\n        data_augmentation (bool): Flag indicating whether to apply data augmentation.\n\n    Returns:\n        torch.utils.data.DataLoader: DataLoader for the training dataset.\n        torch.utils.data.DataLoader: DataLoader for the validation dataset.\n    \"\"\"\n\n    # Define transformation for data without augmentation\n    without_augmentation_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    # Load the original training dataset\n    train_dataset = datasets.ImageFolder(root=train_data_folder, transform=without_augmentation_transform)\n\n    # Shuffle the dataset\n    indices = list(range(len(train_dataset)))\n    np.random.shuffle(indices)\n\n    # Calculate the size of the validation set (20% of the training data)\n    val_size = int(0.2 * len(train_dataset))\n\n    # Calculate the number of samples per class for validation\n    num_classes = len(train_dataset.classes)\n    val_size_per_class = val_size // num_classes\n\n    # Initialize lists to store indices for training and validation\n    train_indices = []\n    val_indices = []\n\n    # Iterate through each class to select validation samples\n    for class_idx in range(num_classes):\n        class_indices = [i for i in indices if train_dataset.targets[i] == class_idx]\n        val_indices.extend(class_indices[:val_size_per_class])\n        train_indices.extend(class_indices[val_size_per_class:])\n\n    if data_augmentation:\n        # Define data augmentation transforms for training data\n        train_transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.Resize((224, 224)),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n\n        # Create PyTorch data loaders for the initial dataset\n        train_loader = DataLoader(Subset(train_dataset, train_indices), batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(Subset(train_dataset, val_indices), batch_size=batch_size, shuffle=True)\n\n        # Create DataLoader for transformed dataset\n        transformed_dataset = datasets.ImageFolder(root=train_data_folder, transform=train_transform)\n        transformed_loader = DataLoader(Subset(transformed_dataset, train_indices), batch_size=batch_size, shuffle=True)\n\n        # Concatenate transformed datasets\n        combined_train_dataset = ConcatDataset([train_loader.dataset, transformed_loader.dataset])\n\n        # Define data loader for combined datasets\n        train_loader = DataLoader(dataset=combined_train_dataset, batch_size=batch_size, shuffle=True)\n    else:\n        # Create PyTorch data loaders for the initial dataset\n        train_loader = DataLoader(Subset(train_dataset, train_indices), batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(Subset(train_dataset, val_indices), batch_size=batch_size, shuffle=True)\n\n    return train_loader, val_loader\n","metadata":{"id":"uspMAjU8uyO_","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define training function\ndef train(model, train_loader, optimizer, criterion, device):\n    \"\"\"\n    Train the model on the training dataset.\n\n    Args:\n        model (torch.nn.Module): The neural network model to be trained.\n        train_loader (torch.utils.data.DataLoader): DataLoader for the training dataset.\n        optimizer (torch.optim.Optimizer): Optimizer used for training.\n        criterion (torch.nn.modules.loss._Loss): Loss function.\n        device (torch.device): Device to perform training on (cuda or cpu).\n\n    Returns:\n        float: Average training loss.\n        float: Training accuracy.\n    \"\"\"\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    model.to(device)\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct / total\n    return train_loss, train_accuracy\n\n# Define testing function\ndef validate(model, val_loader, criterion, device):\n    \"\"\"\n    Validate the model on the validation dataset.\n\n    Args:\n        model (torch.nn.Module): The neural network model to be evaluated.\n        val_loader (torch.utils.data.DataLoader): DataLoader for the validation dataset.\n        criterion (torch.nn.modules.loss._Loss): Loss function.\n        device (torch.device): Device to perform evaluation on (cuda or cpu).\n\n    Returns:\n        float: Average validation loss.\n        float: Validation accuracy.\n    \"\"\"\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    model.to(device)\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_loss = running_loss / len(val_loader)\n    val_accuracy = correct / total\n    return val_loss, val_accuracy\n","metadata":{"id":"y1kZMrtOuyPB","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def arguments(num_filters, batch_size, activation, filter_organization, batch_normalization, data_augmentation,\n              dropout_value, dense_neurons, lr, num_epochs, freeze_option, freeze_index):\n    \"\"\"\n    Set up and train the Fine-Tuned ResNet model with specified parameters.\n\n    Args:\n        num_filters (int): Number of filters.\n        batch_size (int): Batch size for training.\n        activation (str): Activation function to use.\n        filter_organization (str): Filter organization strategy.\n        batch_normalization (str): Whether to use batch normalization ('Yes' or 'No').\n        data_augmentation (str): Whether to use data augmentation ('Yes' or 'No').\n        dropout_value (float): Dropout value.\n        dense_neurons (int): Number of neurons in the dense layer.\n        lr (float): Learning rate.\n        num_epochs (int): Number of epochs to train.\n        freeze_option (str): Option for freezing layers.\n        freeze_index (int): Index for freezing layers.\n\n    Returns:\n        None\n    \"\"\"\n    # Set random seed\n    torch.manual_seed(42)\n\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # Define training data folder\n    train_data_folder = '/kaggle/input/inaturalist/inaturalist_12K/train'\n\n    # Load training dataset\n    train_dataset = datasets.ImageFolder(root=train_data_folder)\n\n    # Model parameters\n    input_channels = 3\n    num_classes = len(train_dataset.classes)\n\n    # Convert batch_normalization and data_augmentation strings to boolean values\n    batch_normalization_val = batch_normalization == \"Yes\"\n    data_augmentation_val = data_augmentation == \"Yes\"\n\n    # Load train and validation data\n    train_loader, val_loader = data_loader(train_data_folder, batch_size, data_augmentation_val)\n\n    # Create model instance\n    model = FineTunedResNet(num_classes, dense_neurons, activation, batch_normalization_val, dropout_value,\n                             freeze_option, freeze_index)\n\n    # Loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr)\n\n    # Training loop\n    for epoch in range(num_epochs):\n        # Initialize tqdm progress bars for training and validation\n        train_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} - Training', leave=False)\n\n        # Training loop\n        train_loss, train_accuracy = train(model, train_loader, optimizer, criterion, device)\n\n        # Validation loop\n        val_loss, val_accuracy = validate(model, val_loader, criterion, device)\n\n        # Log to Weights & Biases\n        wandb.log({\n            \"Epoch\": epoch + 1,\n            \"Train_Accuracy\": train_accuracy,\n            \"Train_Loss\": train_loss,\n            \"Val_Accuracy\": val_accuracy,\n            \"Val_Loss\": val_loss\n        })\n\n        # Print epoch results\n        print(f\"Epoch {epoch+1}/{num_epochs},\\n Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f},\\n Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n","metadata":{"id":"dRrymPbHuyPC","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"id":"UBLX0DzIuyPE","outputId":"9c91b6d0-3e77-48c2-f329-08d0a56050b1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sweep for fine tuning the pretrained Resnet Model\n\nsweep_config = {\n    'method': 'bayes',\n    'name'  : 'Train Dataset Part B',\n    'metric': {'goal': 'maximize', 'name': 'Val_Accuracy'},\n    'parameters': {\n        'num_filters': {'values': [32, 64, 128]},\n        'activation': {'values': ['relu', 'gelu', 'silu', 'mish']},\n        'filter_organization': {'values': ['same', 'double', 'halve']},\n        'batch_normalization': {'values': ['Yes','No']},\n        'dropout_value': {'values': [0.2,0.3]},\n        'learning_rate': {'values': [0.001,0.0001]},\n        'num_epochs': {'values': [5,10]},\n        'dense_neurons': {'values': [128, 256, 512, 1024]},\n        'batch_size': {'values': [32,64]},\n        'freeze_option': {'values': ['all_except_last' ,'except_first','up_to']},\n        'freeze_index': {'values': [2,3,4]},\n        'data_augmentation': {'values': ['Yes','No']}\n    }\n}\n\n\n\n\n# Create sweep\nsweep_id = wandb.sweep(sweep=sweep_config, project=\"DL_Assignment_2_CS23M046\")\n\n","metadata":{"id":"rVFEFbORuyPF","outputId":"1390fe03-ce20-49db-e187-b02c6ba1850c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def main():\n\n    # Initialize wandb\n    with wandb.init() as run:\n\n        config = wandb.config\n\n        run_name=\"Resnet-activation_\"+str(config.activation)+\"_freeze_\"+str(config.freeze_option)+\"_num_filters_\"+str(config.num_filters)+\"_dense_\"+str(config.dense_neurons)+\"_lr_\"+str(config.learning_rate)\n        wandb.run.name=run_name\n\n        arguments(config.num_filters,config.batch_size,config.activation,config.filter_organization,config.batch_normalization,config.data_augmentation,config.dropout_value,config.dense_neurons,config.learning_rate,config.num_epochs,config.freeze_option,config.freeze_index)\n\n\n# Run sweep\nwandb.agent(sweep_id, function=main, count=5)\n\nwandb.finish()","metadata":{"colab":{"referenced_widgets":[""]},"id":"X95kUGxOuyPG","outputId":"f96b456d-06ab-4ca6-8499-e7579b93c7e4","trusted":true},"execution_count":null,"outputs":[]}]}