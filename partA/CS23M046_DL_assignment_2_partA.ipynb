{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7998295,"sourceType":"datasetVersion","datasetId":4709619}],"dockerImageVersionId":30673,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ****Question-1****","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNN(nn.Module):\n    def __init__(self, input_shape, num_classes, num_filters, filter_size, activation_conv, activation_dense, num_neurons_dense):\n        super(CNN, self).__init__()\n        self.conv_layers = self._create_conv_layers(input_shape[0], num_filters, filter_size, activation_conv)\n        self.fc_layers = nn.Sequential(\n            nn.Linear(256 * 7 * 7, num_neurons_dense),\n            activation_dense,\n            nn.Linear(num_neurons_dense, num_classes)\n        )\n\n    def _create_conv_layers(self, input_channels, num_filters, filter_size, activation_conv):\n        layers = []\n        in_channels = input_channels\n        for _ in range(5):  # Reduced to 5 convolutional layers\n            layers += [\n                nn.Conv2d(in_channels, num_filters, filter_size, padding=1),\n                activation_conv,\n                nn.MaxPool2d(kernel_size=2, stride=2)\n            ]\n            in_channels = num_filters\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_layers(x)\n        return x\n\n# Example parameters\ninput_shape = (3, 224, 224)  # Example shape compatible with iNaturalist dataset\nnum_classes = 10  # Number of classes in iNaturalist dataset\nnum_filters = 32  # Number of filters in convolutional layers\nfilter_size = 3  # Size of filters\n\n# Define activation functions for convolutional and dense layers\nactivation_conv = nn.ReLU(inplace=True)  # Activation function for convolutional layers\nactivation_dense = nn.ReLU(inplace=True)  # Activation function for dense layer\n\nnum_neurons_dense = 1024  # Number of neurons in dense layer\n\n# Create the model\nmodel = CNN(input_shape, num_classes, num_filters, filter_size, activation_conv, activation_dense, num_neurons_dense)\n\n# Display model summary\nprint(model)\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-02T08:51:00.057648Z","iopub.execute_input":"2024-04-02T08:51:00.058448Z","iopub.status.idle":"2024-04-02T08:51:04.609539Z","shell.execute_reply.started":"2024-04-02T08:51:00.058397Z","shell.execute_reply":"2024-04-02T08:51:04.608534Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"CNN(\n  (conv_layers): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (10): ReLU(inplace=True)\n    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc_layers): Sequential(\n    (0): Linear(in_features=12544, out_features=1024, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Linear(in_features=1024, out_features=10, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Question-2**","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import ConcatDataset\nimport torch.nn.functional as F\nimport numpy as np\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:21:31.183752Z","iopub.execute_input":"2024-04-02T09:21:31.184652Z","iopub.status.idle":"2024-04-02T09:21:31.189696Z","shell.execute_reply.started":"2024-04-02T09:21:31.184619Z","shell.execute_reply":"2024-04-02T09:21:31.188780Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Define CNN architecture\nclass CNN(nn.Module):\n    def __init__(self, input_channels, num_classes, num_filters):\n        super(CNN, self).__init__()\n\n        self.conv_layers = nn.ModuleList()  # ModuleList to store the convolutional layers\n\n        # Define the convolutional layers dynamically using a loop\n        in_channels = input_channels\n        for filters in num_filters:\n            self.conv_layers.append(nn.Conv2d(in_channels, filters, kernel_size=3, padding=1))\n            self.conv_layers.append(nn.ReLU())  # Add ReLU activation\n            self.conv_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            in_channels = filters\n\n        self.fc1 = nn.Linear(num_filters[-1] * 7 * 7, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        for layer in self.conv_layers:\n            x = layer(x)\n\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # Exclude batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n\n# Define data augmentation transforms for training data\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(),\n    transforms.RandomRotation(10),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Define data augmentation transforms for validation data\nval_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n])\n\n# Define the original training dataset\ntrain_original_dataset = datasets.ImageFolder(root='/kaggle/input/inaturalist/inaturalist_12K/train', transform=train_transform)\n\n# Define the original validation dataset\nval_original_dataset = datasets.ImageFolder(root='/kaggle/input/inaturalist/inaturalist_12K/val', transform=val_transform)\n\n# Concatenate original training dataset with augmented training dataset\ncombined_train_dataset = ConcatDataset([train_original_dataset, train_original_dataset])  # You can repeat train_original_dataset multiple times as needed\n\n# Concatenate original validation dataset with augmented validation dataset\ncombined_val_dataset = ConcatDataset([val_original_dataset, val_original_dataset])  # You can repeat val_original_dataset multiple times as needed\n\n# Define data loaders for combined datasets\ntrain_loader = DataLoader(dataset=combined_train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(dataset=combined_val_dataset, batch_size=64, shuffle=False)\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:21:35.597250Z","iopub.execute_input":"2024-04-02T09:21:35.597593Z","iopub.status.idle":"2024-04-02T09:21:40.374578Z","shell.execute_reply.started":"2024-04-02T09:21:35.597566Z","shell.execute_reply":"2024-04-02T09:21:40.373737Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"\n# Define training function\ndef train(model, train_loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct / total\n    return train_loss, train_accuracy\n\n# Define testing function\ndef test(model, test_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    test_loss = running_loss / len(test_loader)\n    test_accuracy = correct / total\n    return test_loss, test_accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:21:45.411823Z","iopub.execute_input":"2024-04-02T09:21:45.412670Z","iopub.status.idle":"2024-04-02T09:21:45.423391Z","shell.execute_reply.started":"2024-04-02T09:21:45.412640Z","shell.execute_reply":"2024-04-02T09:21:45.422345Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# Model parameters\ninput_channels = 3\nnum_classes = len(train_dataset.classes)\nnum_filters = [32, 64, 128, 256, 128]\n\n# Create model instance\nmodel = CNN(input_channels, num_classes, num_filters).to(device)\n\n# Loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nnum_epochs = 5\nfor epoch in range(num_epochs):\n    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion, device)\n    test_loss, test_accuracy = test(model, val_loader , criterion, device)\n    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-02T09:22:04.430373Z","iopub.execute_input":"2024-04-02T09:22:04.430726Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/5, Train Loss: 2.1917, Train Accuracy: 0.1870, Test Loss: 2.1498, Test Accuracy: 0.2255\nEpoch 2/5, Train Loss: 2.0468, Train Accuracy: 0.2624, Test Loss: 1.9622, Test Accuracy: 0.3070\n","output_type":"stream"}]}]}