{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7998295,"sourceType":"datasetVersion","datasetId":4709619}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ****Question-1****","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass CNN(nn.Module):\n    def __init__(self, input_shape, num_classes, num_filters, filter_size, activation_conv, activation_dense, num_neurons_dense):\n        super(CNN, self).__init__()\n        self.conv_layers = self._create_conv_layers(input_shape[0], num_filters, filter_size, activation_conv)\n        self.fc_layers = nn.Sequential(\n            nn.Linear(256 * 7 * 7, num_neurons_dense),\n            activation_dense,\n            nn.Linear(num_neurons_dense, num_classes)\n        )\n\n    def _create_conv_layers(self, input_channels, num_filters, filter_size, activation_conv):\n        layers = []\n        in_channels = input_channels\n        for _ in range(5):  # Reduced to 5 convolutional layers\n            layers += [\n                nn.Conv2d(in_channels, num_filters, filter_size, padding=1),\n                activation_conv,\n                nn.MaxPool2d(kernel_size=2, stride=2)\n            ]\n            in_channels = num_filters\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc_layers(x)\n        return x\n\n# Example parameters\ninput_shape = (3, 224, 224)  # Example shape compatible with iNaturalist dataset\nnum_classes = 10  # Number of classes in iNaturalist dataset\nnum_filters = 32  # Number of filters in convolutional layers\nfilter_size = 3  # Size of filters\n\n# Define activation functions for convolutional and dense layers\nactivation_conv = nn.ReLU(inplace=True)  # Activation function for convolutional layers\nactivation_dense = nn.ReLU(inplace=True)  # Activation function for dense layer\n\nnum_neurons_dense = 1024  # Number of neurons in dense layer\n\n# Create the model\nmodel = CNN(input_shape, num_classes, num_filters, filter_size, activation_conv, activation_dense, num_neurons_dense)\n\n# Display model summary\nprint(model)\n\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-04T04:43:02.987526Z","iopub.execute_input":"2024-04-04T04:43:02.987875Z","iopub.status.idle":"2024-04-04T04:43:07.427164Z","shell.execute_reply.started":"2024-04-04T04:43:02.987848Z","shell.execute_reply":"2024-04-04T04:43:07.426114Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"CNN(\n  (conv_layers): Sequential(\n    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (4): ReLU(inplace=True)\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (7): ReLU(inplace=True)\n    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (9): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (10): ReLU(inplace=True)\n    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (12): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc_layers): Sequential(\n    (0): Linear(in_features=12544, out_features=1024, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Linear(in_features=1024, out_features=10, bias=True)\n  )\n)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Question-2**","metadata":{}},{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import ConcatDataset\nimport torch.nn.functional as F\nfrom PIL import Image\nimport os\nimport random\nfrom collections import defaultdict  # Import defaultdict\nimport numpy as np\nfrom torch.utils.data import random_split\nfrom torch.utils.data import Subset\nimport wandb\nfrom tqdm import tqdm\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:43:07.428789Z","iopub.execute_input":"2024-04-04T04:43:07.429167Z","iopub.status.idle":"2024-04-04T04:43:11.496872Z","shell.execute_reply.started":"2024-04-04T04:43:07.429130Z","shell.execute_reply":"2024-04-04T04:43:11.496093Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"\n# Define CNN architecture\nclass CNN(nn.Module):\n    def __init__(self, input_channels, num_classes,num_filters,filter_organization,filter_size,activation,batch_normalization,dropout_value,num_nuerons):\n        super(CNN, self).__init__()\n\n        self.conv_layers = nn.ModuleList()  # ModuleList to store the convolutional layers\n\n        # Define the convolutional layers dynamically using a loop\n        padding=1\n        \n        in_channels = input_channels\n        \n        out_size = 224\n        for i in range(5):\n            self.conv_layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=filter_size, padding=padding))\n            if batch_normalization:\n                self.conv_layers.append(nn.BatchNorm2d(num_filters))\n            if activation == 'relu':\n                self.conv_layers.append(nn.ReLU())  # Add ReLU activation\n            elif activation == 'gelu':\n                self.conv_layers.append(nn.GELU())  # Add GELU activation\n            elif activation == 'silu':\n                self.conv_layers.append(nn.SiLU())  # Add SiLU activation\n            elif activation == 'mish':\n                self.conv_layers.append(Mish())  # Add Mish activation\n            self.conv_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n            \n            if(dropout_value > 0):\n                self.conv_layers.append(nn.Dropout(dropout_value))\n                \n            \n            in_channels = num_filters\n            \n            \n            pool_kernel_size=2\n            pool_stride=2\n            \n            # Update num_filters based on filter_organization\n            if filter_organization == 'double':\n                num_filters = int(num_filters * 2)\n            elif filter_organization == 'halve':\n                num_filters = int(num_filters / 2)\n            elif filter_organization == 'same':\n                num_filters=num_filters\n            \n            \n                    \n            # Update out_size based on pooling parameters\n            out_size = ((out_size + 2 * padding - pool_kernel_size) // pool_stride )+ 1\n                \n                \n        \n                \n        \n        \n\n        self.fc1 = nn.Linear(in_channels * (out_size-1) * (out_size-1), num_nuerons)\n        self.fc2 = nn.Linear(num_nuerons, num_classes)\n\n    def forward(self, x):\n        for layer in self.conv_layers:\n            x = layer(x)\n\n        x = x.view(-1, self.num_flat_features(x))\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n    def num_flat_features(self, x):\n        size = x.size()[1:]  # Exclude batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n    \n    \nclass Mish(nn.Module):\n    def forward(self, x):\n        return x * torch.tanh(F.softplus(x))\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:43:11.498015Z","iopub.execute_input":"2024-04-04T04:43:11.498307Z","iopub.status.idle":"2024-04-04T04:43:11.514332Z","shell.execute_reply.started":"2024-04-04T04:43:11.498281Z","shell.execute_reply":"2024-04-04T04:43:11.513366Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def data_loader(train_data_folder,batch_size,data_augmentation):\n    \n    \n    without_augmentation_transform =  transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n    # Load the original training dataset\n    train_dataset = datasets.ImageFolder(root=train_data_folder, transform=without_augmentation_transform)\n\n    # Shuffle the dataset\n    indices = list(range(len(train_dataset)))\n    np.random.shuffle(indices)\n    \n    # Calculate the size of the validation set (20% of the training data)\n    val_size = int(0.2 * len(train_dataset))\n    \n    # Calculate the number of samples per class for validation\n    num_classes = len(train_dataset.classes)\n    val_size_per_class = val_size // num_classes\n    \n    # Initialize lists to store indices for training and validation\n    train_indices = []\n    val_indices = []\n    \n    # Iterate through each class to select validation samples\n    for class_idx in range(num_classes):\n        class_indices = [i for i in indices if train_dataset.targets[i] == class_idx]\n        val_indices.extend(class_indices[:val_size_per_class])\n        train_indices.extend(class_indices[val_size_per_class:])\n    \n\n\n    if data_augmentation:\n\n        # Define data augmentation transforms for training data\n        train_transform = transforms.Compose([\n            transforms.RandomHorizontalFlip(),\n            transforms.Resize((224, 224)),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n        ])\n        \n        # Create PyTorch data loaders for the initial dataset\n        train_loader = DataLoader(Subset(train_dataset, train_indices), batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(Subset(train_dataset, val_indices), batch_size=batch_size, shuffle=True)\n        \n        \n        transformed_dataset = datasets.ImageFolder(root=train_data_folder, transform=train_transform)\n        transformed_loader = DataLoader(Subset(transformed_dataset, train_indices), batch_size=batch_size, shuffle=True)\n        \n\n\n\n\n  \n        \n\n\n        # Concatenate transformed datasets\n        combined_train_dataset = ConcatDataset([train_loader.dataset,transformed_loader.dataset ])  # You can repeat train_dataset_transformed multiple times as needed\n        \n        # Define data loaders for combined datasets\n        train_loader = DataLoader(dataset=combined_train_dataset, batch_size=batch_size, shuffle=True)\n    \n\n    else:\n        # Create PyTorch data loaders for the initial dataset\n        train_loader = DataLoader(Subset(train_dataset, train_indices), batch_size=batch_size, shuffle=True)\n        val_loader = DataLoader(Subset(train_dataset, val_indices), batch_size=batch_size, shuffle=True)\n\n\n    \n    return train_loader , val_loader","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:43:11.517229Z","iopub.execute_input":"2024-04-04T04:43:11.517623Z","iopub.status.idle":"2024-04-04T04:43:11.530507Z","shell.execute_reply.started":"2024-04-04T04:43:11.517597Z","shell.execute_reply":"2024-04-04T04:43:11.529729Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\n# Define training function\ndef train(model, train_loader, optimizer, criterion, device):\n    model.train()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    for inputs, labels in train_loader:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        running_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n    train_loss = running_loss / len(train_loader)\n    train_accuracy = correct / total\n    return train_loss, train_accuracy\n\n# Define testing function\ndef validate(model, val_loader, criterion, device):\n    model.eval()\n    running_loss = 0.0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for inputs, labels in val_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    val_loss = running_loss / len(val_loader)\n    val_accuracy = correct / total\n    return val_loss, val_accuracy\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:43:11.531428Z","iopub.execute_input":"2024-04-04T04:43:11.531691Z","iopub.status.idle":"2024-04-04T04:43:11.544545Z","shell.execute_reply.started":"2024-04-04T04:43:11.531669Z","shell.execute_reply":"2024-04-04T04:43:11.543688Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def arguments(num_filters,batch_size,activation,filter_organization,batch_normalization,data_augmentation,dropout_value,num_nuerons,lr,num_epochs):\n    \n\n\n    # Set random seed\n    torch.manual_seed(42)\n    \n    \n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    train_data_folder='/kaggle/input/inaturalist/inaturalist_12K/train'\n\n    train_dataset = datasets.ImageFolder(root=train_data_folder)\n    \n\n\n    \n\n    # Model parameters\n    input_channels = 3\n    num_classes = len(train_dataset.classes)\n\n\n\n\n\n\n\n    filter_size=3\n    if(batch_normalization == \"Yes\"):\n        batch_normalization_val=True\n    elif(batch_normalization == \"No\"):\n        batch_normalization_val=False\n        \n    \n    \n    \n    \n    if(data_augmentation == \"Yes\"):\n        data_augmentation_val=True\n    elif(data_augmentation == \"No\"):\n        data_augmentation_val=False\n    \n    train_loader , val_loader=data_loader(train_data_folder,batch_size,data_augmentation_val)\n\n    # Create model instance\n    model = CNN(input_channels, num_classes, num_filters,filter_organization,filter_size,activation,batch_normalization_val,dropout_value,num_nuerons).to(device)\n\n    # Loss function and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr)\n\n\n\n    # Training loop\n  \n    for epoch in range(num_epochs):\n        epoch_progress = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n        train_loss, train_accuracy = train(model, train_loader, optimizer, criterion, device)\n        val_loss, val_accuracy = validate(model, val_loader , criterion, device)\n        \n        # Log to Weights & Biases\n        wandb.log({\n            \"Epoch\": epoch + 1,\n            \"Train_Accuracy\": train_accuracy,\n            \"Train_Loss\": train_loss,\n            \"Val_Accuracy\": val_accuracy,\n            \"Val_Loss\": val_loss\n        })\n        print(f\"Epoch {epoch+1}/{num_epochs},\\n Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f},\\n Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:43:11.545720Z","iopub.execute_input":"2024-04-04T04:43:11.546056Z","iopub.status.idle":"2024-04-04T04:43:11.559481Z","shell.execute_reply.started":"2024-04-04T04:43:11.546026Z","shell.execute_reply":"2024-04-04T04:43:11.558655Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"wandb.login()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:43:11.560492Z","iopub.execute_input":"2024-04-04T04:43:11.560797Z","iopub.status.idle":"2024-04-04T04:43:33.717366Z","shell.execute_reply.started":"2024-04-04T04:43:11.560768Z","shell.execute_reply":"2024-04-04T04:43:33.716364Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"sweep_config = {\n    'method': 'bayes',\n    'metric': {'goal': 'maximize', 'name': 'Val_Accuracy'},\n    'parameters': {\n        'num_filters': {'values': [32, 64, 128]},\n        'activation': {'values': ['relu', 'gelu', 'silu', 'mish']},\n        'filter_organization': {'values': ['same', 'double', 'halve']},\n        'batch_normalization': {'values': ['Yes', 'No']},\n        'dropout_value': {'values': [0.2, 0.3]},\n        'learning_rate': {'values': [0.001, 0.0001]},\n        'num_epochs': {'value': 5},\n        'dense_neurons': {'values': [128, 256, 512, 1024]},\n        'batch_size': {'values': [32, 64]},\n        'data_augmentation': {'values': ['Yes', 'No']}\n    }\n}\n\n\n\n\n\n\n# Create sweep\nsweep_id = wandb.sweep(sweep=sweep_config, project=\"DL_Assignment_2_CS23M046\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:43:33.718687Z","iopub.execute_input":"2024-04-04T04:43:33.719570Z","iopub.status.idle":"2024-04-04T04:43:46.677373Z","shell.execute_reply.started":"2024-04-04T04:43:33.719535Z","shell.execute_reply":"2024-04-04T04:43:46.676490Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Create sweep with ID: a7yi5gls\nSweep URL: https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/sweeps/a7yi5gls\n","output_type":"stream"}]},{"cell_type":"code","source":"def main():\n    \n    # Initialize wandb\n    with wandb.init() as run:\n\n        config = wandb.config\n        \n        run_name=\"activation_\"+str(config.activation)+\"_num_filters_\"+str(config.num_filters)+\"_dense_\"+str(config.dense_neurons)+\"_lr_\"+str(config.learning_rate)\n        wandb.run.name=run_name\n        \n        arguments(config.num_filters,config.batch_size,config.activation,config.filter_organization,config.batch_normalization,config.data_augmentation,config.dropout_value,config.dense_neurons,config.learning_rate,config.num_epochs)\n\n\n\n\n\n\n# Run sweep\nwandb.agent(sweep_id, function=main, count=1)\n\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:43:46.678786Z","iopub.execute_input":"2024-04-04T04:43:46.679075Z","iopub.status.idle":"2024-04-04T04:59:05.548340Z","shell.execute_reply.started":"2024-04-04T04:43:46.679049Z","shell.execute_reply":"2024-04-04T04:59:05.547284Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: pm3lrtuq with config:\n\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: gelu\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_normalization: No\n\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 64\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdata_augmentation: No\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdense_neurons: 256\n\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout_value: 0.3\n\u001b[34m\u001b[1mwandb\u001b[0m: \tfilter_organization: same\n\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0001\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_epochs: 5\n\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_filters: 128\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs23m046\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240404_044348-pm3lrtuq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/runs/pm3lrtuq' target=\"_blank\">fast-sweep-1</a></strong> to <a href='https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/sweeps/a7yi5gls' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/sweeps/a7yi5gls</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View sweep at <a href='https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/sweeps/a7yi5gls' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/sweeps/a7yi5gls</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/runs/pm3lrtuq' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/runs/pm3lrtuq</a>"},"metadata":{}},{"name":"stderr","text":"Epoch 1/5:   0%|          | 0/126 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch 1/5,\n Train Loss: 2.2574, Train Accuracy: 0.1508,\n Val Loss: 2.1861, Val Accuracy: 0.2000\n","output_type":"stream"},{"name":"stderr","text":"\n                                                  \u001b[A\r","output_type":"stream"},{"name":"stdout","text":"Epoch 2/5,\n Train Loss: 2.1380, Train Accuracy: 0.2235,\n Val Loss: 2.0857, Val Accuracy: 0.2568\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/5:   0%|          | 0/126 [00:00<?, ?it/s]\n                                                  \u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 3/5,\n Train Loss: 2.0354, Train Accuracy: 0.2671,\n Val Loss: 2.0332, Val Accuracy: 0.2779\n","output_type":"stream"},{"name":"stderr","text":"\n                                                  \u001b[A\r","output_type":"stream"},{"name":"stdout","text":"Epoch 4/5,\n Train Loss: 1.9763, Train Accuracy: 0.2892,\n Val Loss: 2.0051, Val Accuracy: 0.2965\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/5:   0%|          | 0/126 [00:00<?, ?it/s]\n                                                  \u001b[A","output_type":"stream"},{"name":"stdout","text":"Epoch 5/5,\n Train Loss: 1.9307, Train Accuracy: 0.3116,\n Val Loss: 1.9767, Val Accuracy: 0.2910\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Train_Accuracy</td><td>▁▄▆▇█</td></tr><tr><td>Train_Loss</td><td>█▅▃▂▁</td></tr><tr><td>Val_Accuracy</td><td>▁▅▇██</td></tr><tr><td>Val_Loss</td><td>█▅▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train_Accuracy</td><td>0.31165</td></tr><tr><td>Train_Loss</td><td>1.93069</td></tr><tr><td>Val_Accuracy</td><td>0.29095</td></tr><tr><td>Val_Loss</td><td>1.97674</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">fast-sweep-1</strong> at: <a href='https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/runs/pm3lrtuq' target=\"_blank\">https://wandb.ai/cs23m046/DL_Assignment_2_CS23M046/runs/pm3lrtuq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_044348-pm3lrtuq/logs</code>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Question 4","metadata":{}},{"cell_type":"code","source":"# # Define testing function\n# def test(model, test_loader, criterion, device):\n#     model.eval()\n#     running_loss = 0.0\n#     correct = 0\n#     total = 0\n#     with torch.no_grad():\n#         for inputs, labels in test_loader:\n#             inputs, labels = inputs.to(device), labels.to(device)\n#             outputs = model(inputs)\n#             loss = criterion(outputs, labels)\n#             running_loss += loss.item()\n#             _, predicted = torch.max(outputs, 1)\n#             total += labels.size(0)\n#             correct += (predicted == labels).sum().item()\n#     test_loss = running_loss / len(test_loader)\n#     test_accuracy = correct / total\n#     return test_loss, test_accuracy","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:59:05.550874Z","iopub.execute_input":"2024-04-04T04:59:05.551186Z","iopub.status.idle":"2024-04-04T04:59:05.556081Z","shell.execute_reply.started":"2024-04-04T04:59:05.551136Z","shell.execute_reply":"2024-04-04T04:59:05.555190Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# without_augmentation_transform =  transforms.Compose([\n#         transforms.Resize((224, 224)),\n#         transforms.ToTensor(),\n#         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n#     ])\n\n# test_data_folder='/kaggle/input/inaturalist/inaturalist_12K/val'\n\n# test_dataset = datasets.ImageFolder(root=test_data_folder,transform=without_augmentation_transform)\n# train_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=True)\n\n# input_channels=3\n# num_classes=10\n# num_filters=32\n# filter_organization='double'\n# filter_size=3\n# activation='relu'\n# batch_normalization=True\n# dropout_value=0.3\n# num_nuerons=128\n\n# # Create model instance\n# best_model = CNN(input_channels, num_classes, num_filters,filter_organization,filter_size,activation,batch_normalization,dropout_value,num_nuerons).to(device)\n\n\n# test_loss, test_accuracy =test(best_model, test_loader, criterion, device)\n\n# print(f'Test Loss :{test_loss}')\n# print(f'Test Accuracy :{test_accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-04-04T04:59:05.557085Z","iopub.execute_input":"2024-04-04T04:59:05.557374Z","iopub.status.idle":"2024-04-04T04:59:05.569689Z","shell.execute_reply.started":"2024-04-04T04:59:05.557350Z","shell.execute_reply":"2024-04-04T04:59:05.568929Z"},"trusted":true},"execution_count":12,"outputs":[]}]}